{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** Please note that it is not good practice to store a pre-rendered notebook externally (e.g. in GitHub or GitLab). This has been provided for training purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A (quick) Introduction to Python**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will dive into some Exploratory Data Analysis using Python. Our 3 aims here will be -\n",
    "- Reading in and exporting data\n",
    "- Dataframes and cleaning data\n",
    "- Summary statistics and basic manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd     # present working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to import the Python packages that we will be using in this session:\n",
    "- `pandas` (go-to tools for data analysis and manipulation)\n",
    "- `numpy` (maths functions and working with arrays)\n",
    "- `matplotlib` (baseline visualisation package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to download 2 datasets for this session. The URLs can be found in the `url_links.txt` file in the `data` folder.\n",
    "\n",
    "Unzip the broadband data and copy all data files into the `data` folder.\n",
    "\n",
    "The following cell will rename the files and remove one file that is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadband_name = \"data/202209_fixed_oa_res_coverage_r02.csv\"     # check that this is the name of your OFCOM file\n",
    "new_broadband_name = \"data/ofcom_coverage_202209.csv\"\n",
    "\n",
    "lookup_name = \"data/OA21_RGN22_LU.csv\"      # check that this is the name of your lookup file\n",
    "new_lookup_name = \"data/ons_region_lookup_2022.csv\"\n",
    "\n",
    "popest_name = \"data/sape23dt10dmid2020coaunformattedsyoaestimatesnortheast.xlsx\"      # check that this is the name of your lookup file\n",
    "new_popest_name = \"data/pop_estimates_neengland_2020.xlsx\"\n",
    "\n",
    "file_to_drop = \"data/202209_fixed_oa_coverage_r02.csv\"\n",
    "\n",
    "if os.path.exists(file_to_drop):\n",
    "    os.remove(file_to_drop)\n",
    "\n",
    "if os.path.exists(broadband_name):\n",
    "    os.rename(broadband_name, new_broadband_name)\n",
    "    \n",
    "if os.path.exists(lookup_name):\n",
    "    os.rename(lookup_name, new_lookup_name)\n",
    "    \n",
    "if os.path.exists(popest_name):\n",
    "    os.rename(popest_name, new_popest_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a versatile library which is capable of importing and interpreting multiple file formats ranging from flat files, databases and even parquet files. Common **read** functions include -\n",
    "- `pd.read_csv()`\n",
    "- `pd.read_excel()`\n",
    "- `pd.read_json()`\n",
    "- ...\n",
    "\n",
    "Note that some will require additional installations. In particular, Excel files can be a bit more involved because we need to consider which sheet(s) we require and handle any header rows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = pd.read_csv(\"data/ofcom_coverage_202209.csv\")    # can use a relative path\n",
    "lookup = pd.read_csv(\"data/ons_region_lookup_2022.csv\")\n",
    "# lookup = pd.read_csv(\"data/OA21_RGN22_LU.csv\", dtype={\"rgn22nmw\": \"str\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing an Excel file, we must ensure that we have `openpyxl` installed and/or upgraded to the latest verson i.e.   \n",
    "`pip install openpyxl` *or*   \n",
    "`pip install --upgrade openpyxl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure openpyxl upgraded to latest version (3.1.2)\n",
    "popest = pd.read_excel(\"data/pop_estimates_neengland_2020.xlsx\", sheet_name=\"Mid-2020 Persons\", skiprows=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect our data in a number of ways:\n",
    "- using `.head()` for the top rows (a number can be specified, default is 5)\n",
    "- using `.tail()` as above, for bottom rows\n",
    "- using `.sample()` as above, for random rows (a random seed can be used)\n",
    "- index slicing\n",
    "- using `.loc` to specify rows and columns by name\n",
    "- using `.iloc` to specify rows and columns by index reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 2 rows of our `coverage` data\n",
    "coverage.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the ellipsis (...) masking some of the rows i.e. the dataframe is too big to display by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None     # this setting will now persist\n",
    "coverage.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the values in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lookup.head(2))\n",
    "print(lookup.tail(2))\n",
    "print(lookup.sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the **NaN** values in the `lookup` table. We can summarise the extent of these easily in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These **NaN** values are easily explained: Welsh is not provided for regions outside of Wales. Although fairly arbitrary, we could handle these missing values in a number of ways:\n",
    "- filling all gaps with a value\n",
    "- dropping rows with containing missing values\n",
    "- assign values based on another column (or even an external function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling all gaps with a value\n",
    "lookup[\"rgn22nmw\"].fillna(\"not in Wales\").sample(5, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with missing values (leaves only Welsh OAs, in this example)\n",
    "lookup.dropna(subset=[\"rgn22nmw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a dictionary to the region names in English to provide the Welsh names\n",
    "welsh_regions = {\"London\": \"Llundain\",\n",
    "                 \"North West\": \"Gogledd Orllewin Lloegr\",\n",
    "                 \"Yorkshire and The Humber\": \"Swydd Efrog a'r Humber\",\n",
    "                 \"North East\": \"Gogledd-ddwyrain Lloegr\",\n",
    "                 \"West Midlands\": \"Gorllewin Canolbarth Lloegr\",\n",
    "                 \"East Midlands\": \"Dwyrain Canolbarth Lloegr\",\n",
    "                 \"South West\": \"De-orllewin Lloegr\",\n",
    "                 \"East of England\": \"Dwyrain Lloegr\",\n",
    "                 \"South East\": \"De-ddwyrain Lloegr\",\n",
    "                 \"Wales\": \"Cymru\"}\n",
    "\n",
    "lookup[\"rgn22nmw\"] = lookup[\"rgn22nm\"].map(welsh_regions)\n",
    "lookup.sample(5, random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply a 'treatment' in a bonus example at the end of this notebook!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily slice a dataframe if we know which columns (or rows) we want. We use [] to specify a column name or [[]] for multiple column names. You may also have seen columns being called using the (example) `lookup.oa21cd` notation. This is not considered best practice but it can become useful at times, for example for more complex visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows 10-19 using index slicing\n",
    "popest[9:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column slicing\n",
    "lookup[[\"oa21cd\", \"rgn22nm\"]]\n",
    "\n",
    "# row and column slicing\n",
    "# lookup[:5][[\"oa21cd\", \"rgn22nm\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we decide that there are 2 columns we are interested in, in particular: **SFBB availability (% premises)** and **Gigabit availability (% premises)**. Earlier, we displayed the initial rows of the `coverage` dataframe i.e. `coverage.head()`. We can see that our 2 columns are column FOUR and column ELEVEN. Python uses zero-indexing (where the first column is referenced as column ZERO) so we will need to access column THREE and column TEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index location\n",
    "coverage.iloc[:5, [3,10]]    # i.e. top 5 rows (:5) and a list of column indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can access these rows and columns by name:\n",
    "coverage.loc[:5, [\"SFBB availability (% premises)\", \"Gigabit availability (% premises)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also investigate the data structure: columns, rows, data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup.shape      # numer of rows and columns as a tuple\n",
    "#len(lookup)     # number of rows only\n",
    "#len(lookup.columns)     # number of columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements of a tuple can be accessed by index\n",
    "lookup.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the use of f-string to embed variable values within a string\n",
    "print(f\"In the lookup table, there are {lookup.shape[0]} rows and {lookup.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup.columns.tolist()    # functions and/or methods can often be chained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage.dtypes\n",
    "#coverage.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter our dataframe in terms of data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage.select_dtypes(include=[\"object\", \"float64\"]).head(2)\n",
    "#coverage.select_dtypes(exclude=[\"int\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate summary statistics for the whole dataframe (numerical variables only) or specific columns. Here a negative skew in the distribution of Gigabit coverage by output area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage[\"Gigabit availability (% premises)\"].describe()\n",
    "\n",
    "#coverage.describe()    # for ALL columns with numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Descriptive statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas** is a huge library of methods for us to interrogate diverse data types with flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *range and averages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Gigabit availability (% premises)\"\n",
    "print(f\"Minimum: {coverage[col].min()}\")\n",
    "print(f\"Maximum: {coverage[col].max()}\")\n",
    "print(f\"Median: {coverage[col].median()}\")\n",
    "print(f\"Other quantiles: \\n{coverage[col].quantile(q=[0.25, 0.75]).values}\")\n",
    "print(f\"Mode: {coverage[col].mode()[0]}\")\n",
    "\n",
    "# coverage[col].quantile(q=np.arange(0, 1.1, 0.1))     # deciles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *spread*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage[col].std()      # standard deviation\n",
    "#coverage[col].var()     # variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at visualisations in a later session but it is helpful that `pandas` uses the `matplotlib` library under the hood. This allows for a direct and responsive approach to visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage[\"Gigabit availability (% premises)\"].plot(kind=\"hist\");\n",
    "#coverage[\"Gigabit availability (% premises)\"].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also interrogate columns for unique values and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique variables\n",
    "lookup[\"rgn22nm\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique variables in a given column\n",
    "lookup[\"rgn22nm\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total missing values\n",
    "lookup[\"rgn22nmw\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our work, we might like to look at the prevalence of properties - by output area - with the option of the fastest broadband speeds. We can use binning to create new variables. Two approaches might be useful here:    \n",
    "- `pd.cut` divides values into bins of broadly equal range (or user-defined)   \n",
    "- `pd.qcut` divides values into bins with broadly equal values assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 bins of equal range\n",
    "coverage[\"fast_bin_pct_prems\"] = pd.cut(coverage[\"% of premises with >=300Mbit/s download speed\"], bins=5)\n",
    "coverage[\"fast_bin_pct_prems\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we also want to comment on where output areas sit in terms of numbers of premises with the option of gigabit connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also specify the bin ranges but note the 0 behaviour\n",
    "coverage[\"gig_bin_prems\"] = pd.cut(coverage[\"Number of premises with Gigabit availability\"], bins=coverage[\"Number of premises with Gigabit availability\"].quantile(q=[0,0.25,0.5,0.75,1]))\n",
    "coverage[\"gig_bin_prems\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and `qcut` distributes bin allocation equally\n",
    "coverage[\"gig_qbin_prems\"] = pd.qcut(coverage[\"Number of premises with Gigabit availability\"], q=4, labels=[\"First quartile\", \"Second quartile\", \"Third quartile\", \"Fourth quartile\"])\n",
    "coverage[\"gig_qbin_prems\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting our new features\n",
    "coverage[[\"% of premises with >=300Mbit/s download speed\",\n",
    "          \"fast_bin_pct_prems\",\n",
    "          \"Number of premises with Gigabit availability\",\n",
    "          \"gig_bin_prems\",\n",
    "          \"gig_qbin_prems\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the focus of this session is not visualisation but `matplotlib` makes it easy for us to view the difference in bin allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(nrows=2, figsize=(20,10))\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "coverage[\"fast_bin_pct_prems\"].value_counts().sort_index().plot(kind=\"barh\", xlabel=\"num_OAs\")\n",
    "ax2 = plt.subplot(2,1,2, sharex=ax1)\n",
    "coverage[\"gig_qbin_prems\"].value_counts().sort_index().plot(kind=\"barh\", xlabel=\"num_OAs\", color=\"orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter our data in a number of ways. In the following examples, we will filter by specific conditions both directly and by defining a mask (i.e. binary labelling of each row in terms of it meeting specified condition(s).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison operators can be used to define a condition\n",
    "mask = popest[\"All Ages\"]>=400      # ==, !=, >, >=, <, <=\n",
    "mask.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct filtering by condition\n",
    "popest[popest[\"All Ages\"]>=400]\n",
    "#popest[mask]     # filtering using the mask defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter on a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popest.loc[:, \"OA11CD\"][mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple condition filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to investigate all output areas in Wales with less than 20% of premises with a gigabit option.   \n",
    "We can filter a dataframe by multiple conditions where each condition is encapsulated within () and divided by   \n",
    "- **&** (and)  \n",
    "- **|** (or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as before, binary label for each row\n",
    "mask = (coverage[\"Gigabit availability (% premises)\"]<20) & (coverage[\"output_area\"].str[0]==\"W\")\n",
    "mask.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the rows meeting the conditions directly\n",
    "coverage[(coverage[\"Gigabit availability (% premises)\"]<20) & (coverage[\"output_area\"].str[0]==\"W\")]\n",
    "#coverage[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to repeat this for all output areas in North East England, we can filter these codes using the `.isin()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of OAs in North East and North West England\n",
    "reqd_oas = lookup[\"oa21cd\"][lookup[\"rgn22nm\"].isin([\"North East\"])].tolist()\n",
    "\n",
    "# define conditions, including one as being in the above list\n",
    "low_gig_north_england = coverage[(coverage[\"Gigabit availability (% premises)\"]<20) & (coverage[\"output_area\"].isin(reqd_oas))]\n",
    "low_gig_north_england"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we might want to check that the superfast availability is adequate even though the above areas have fairly poor gigabit coverage. We can use the `sort_values()` method to present the output areas in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_gig_north_england.sort_values(\"SFBB availability (% premises)\")     # ascending order is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_gig_north_england[\"SFBB availability (% premises)\"].hist(grid=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in unpicking the age demographic in the output areas above with 25% superfast coverage or lower. Again, we will want to create a list of these affected OAs.\n",
    "\n",
    "In our example, we will look at the prevalence of younger children. First of all, we create an aggregate a new feature to capture all ages from 0 to 11 in our `popest` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popest.columns = popest.columns.astype(\"str\")\n",
    "popest[\"11_or_under\"] = popest.loc[:, [0,1,2,3,4,5,6,7,8,9,10,11]].sum(axis=1)\n",
    "popest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuln_oas = low_gig_north_england[\"output_area\"][low_gig_north_england[\"SFBB availability (% premises)\"]<=25].tolist()\n",
    "\n",
    "# preview the first 10 items in the list\n",
    "vuln_oas[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrive at a filtered view of our original dataframe - North East England OAs with <20% gigabit and <=25% superfast - ordered by numbers of primary-age pupils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering and sorting\n",
    "ordered_popest = popest[[\"OA11CD\", \"LSOA11CD\", \"All Ages\", \"11_or_under\"]][popest[\"OA11CD\"].isin(vuln_oas)].sort_values(\"11_or_under\", ascending=False)\n",
    "ordered_popest.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_popest[\"pct_11_or_under\"] = ordered_popest[\"11_or_under\"]/ordered_popest[\"All Ages\"]*100\n",
    "ordered_popest.sort_values(\"pct_11_or_under\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we might want to list ALL the output areas by population estimate and by LSOA in turn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note lists to provide parameters for multiple columns\n",
    "popest.iloc[:, :3].sort_values([\"LSOA11CD\", \"All Ages\"], ascending=[True, False]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our UK broadband coverage data. We can use the `.map()` method apply a dictionary to an existing column to generate values in a new column. Here, let's take the initial (country) letter for each output area to create a column of country labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage[\"country\"] = coverage[\"output_area\"].str[0].map({\"E\": \"England\",\n",
    "                                                          \"W\": \"Wales\",\n",
    "                                                          \"S\": \"Scotland\",\n",
    "                                                          \"N\": \"Northern Ireland\"})\n",
    "\n",
    "# this 'random' sample demonstrates the effect of the `.map()` method\n",
    "coverage[[\"output_area\", \"country\"]].sample(5, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns can be renamed\n",
    "coverage.rename(columns={\"country\": \"uk_country\"}, inplace=True)\n",
    "\n",
    "# values can be replaced\n",
    "coverage[\"uk_country\"].replace(\"Northern Ireland\", \"N Ireland\", inplace=True)\n",
    "\n",
    "coverage[\"uk_country\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all 4 countries have been captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage[\"uk_country\"].value_counts()\n",
    "#coverage[\"output_area\"].str[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deleting columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting columns should only be done if absolutely necessary. However, particularly for larger datasets, it can be a useful way of freeing up memory. The following examples are provided for illustration only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we decide that Next Generation Access (NGA) variables are no longer required. We will use the `.filter()` method to identify columns with the *NGA* acronym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage.filter(like=\"NGA\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 'del' in Python 3\n",
    "del coverage[\"% of premises with NGA\"]      # permanent!\n",
    "coverage.filter(like=\"NGA\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the `.drop()` method\n",
    "coverage.drop(columns=[\"Number of premises with NGA\"], inplace=True)\n",
    "coverage.filter(like=\"NGA\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Joining dataframes**  \n",
    "**Note:** In this illustration, we are joining two dataframes with different output area codes (one Census 2011, the other Census 2021). Only those output area codes to feature in both dataframes will be included in an inner join. Consequently, we are using a left join in this illustration to ensure that all existing population estimates are retained, irrespective of output area code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows in each dataframe\n",
    "len(coverage), len(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_incl_region = pd.merge(coverage, lookup, left_on=\"output_area\", right_on=\"oa21cd\", how=\"left\")\n",
    "cir = cov_incl_region.copy()      # for use later on\n",
    "cov_incl_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cov_incl_region.merge(popest, left_on=\"output_area\", right_on=\"OA11CD\", how=\"inner\")     # inner join as we are only working with North-East England population estimates\n",
    "df[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods available in Python to achieve the same (or similar) results. You will want to explore these:   \n",
    "- `pd.merge`   \n",
    "- `pd.concat`   \n",
    "- `pd.join`   \n",
    "- `pd.append`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save this dataframe using the `pd.to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"outputs/broadband_neengland_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aggregations using the `.groupby()` method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.groupby()` method allows us to slice a dataframe by any number of columns and aggregate the values in terms of any number of other columns.\n",
    "\n",
    "So - in this first example, we aggregate the maximum % of premises unable to receive 2Mbit/s download speeds in each LSOA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"LSOA11CD\"])[\"% of premises unable to receive 2Mbit/s\"].agg(\"max\").sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can often leave out the `.agg()` method to achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_incl_region.groupby([\"rgn22nm\"])[\"output_area\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the slice and aggregations can be defined in the form of a dictionary passed to the `.agg()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the chained methods!\n",
    "df.groupby(\"LSOA11CD\").agg({\"Number of premises unable to receive 2Mbit/s\": np.max,\n",
    "                            \"Number of premises with >=300Mbit/s download speed\":\"median\"}).sort_values(\"Number of premises unable to receive 2Mbit/s\", ascending=False).reset_index().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A few other things worth knowing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.copy()` method is useful for creating an independent copy of a dataframe. Without this, you create a view which meaning both will update concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = cir.copy()     # this creates an INDEPENDENT copy\n",
    "copy_df[\"rgn22nm\"].replace({\"North East\": \"NE England\"}, inplace=True)\n",
    "copy_count = len(copy_df[copy_df[\"rgn22nm\"]==\"NE England\"])\n",
    "orig_count = len(cir[cir[\"rgn22nm\"]==\"NE England\"])\n",
    "print(f\"In the copy: {copy_count} rows matching\")\n",
    "print(f\"In the original: {orig_count} rows matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = cir      # this creates a DEPENDENT view\n",
    "copy_df[\"rgn22nm\"].replace({\"North East\": \"NE England\"}, inplace=True)\n",
    "copy_count = len(copy_df[copy_df[\"rgn22nm\"]==\"NE England\"])\n",
    "orig_count = len(cir[cir[\"rgn22nm\"]==\"NE England\"])\n",
    "print(f\"In the copy: {copy_count} rows matching\")\n",
    "print(f\"In the original: {orig_count} rows matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is so critical that `pandas` will return a Copy Warning recommending the use of the `.loc()` method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_incl_region.loc[cov_incl_region[\"rgn22nm\"]==\"North West\", \"rgn22nm\"] = \"NW England\"\n",
    "cov_incl_region.loc[cov_incl_region[\"rgn22nm\"]==\"North East\", \"rgn22nm\"] = \"NE England\"\n",
    "cov_incl_region[\"rgn22nm\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String variables can easily be manipulated. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing <SPACE> with <UNDERSCORE>\n",
    "cov_incl_region[\"rgn22nm\"].str.replace(pat=\" \", repl=\"_\").unique()\n",
    "#cov_incl_region[\"rgn22nm\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewriting in lowercase\n",
    "cov_incl_region[\"rgn22nm\"].str.lower().unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b96ab2eae6bf74f117f63fe3b9d4ead47ac0870b40b8d148db2bf9f9dbf172ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
